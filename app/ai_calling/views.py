"""
AI Calling Views
================
API endpoints for AI-powered calling functionality
"""

from typing import List, Optional
from fastapi import APIRouter, HTTPException, status
from pydantic import BaseModel, Field
import glob
import json
import uuid
import random
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
import time

from app.ai_calling.service import (
    make_outbound_call,
    get_call_data_store,
    gemini_client,
    analyze_conversation_with_gemini,
    ConversationHandler
)
from config import settings
from app.db import db
from app.auth.utils import get_current_user
from fastapi import APIRouter, HTTPException, status, Depends

router = APIRouter()


# ============================================================
# DUMMY CONVERSATION DATA
# ============================================================

DUMMY_CONVERSATIONS = {
    "en-IN": {
        "call_uuid": "dummy-call-{uuid}",
        "start_time": "{start_time}",
        "end_time": "{end_time}",
        "duration_seconds": 52.59,
        "preferred_language": "en-IN",
        "final_language": "en-IN",
        "language_switches": 0,
        "language_history": [],
        "conversation": [
            {
                "speaker": "AI",
                "text": "Hello, I am calling from the finance agency regarding your loan payment. May I know your current payment status?",
                "timestamp": "{timestamp_1}",
                "language": "en-IN"
            },
            {
                "speaker": "User",
                "text": "I will pay day after tomorrow, is it fine?",
                "timestamp": "{timestamp_2}",
                "language": "en-IN"
            },
            {
                "speaker": "AI",
                "text": "Thank you for letting us know. Could you please confirm the specific date you expect to make the payment?",
                "timestamp": "{timestamp_3}",
                "language": "en-IN"
            },
            {
                "speaker": "User",
                "text": "Okay.",
                "timestamp": "{timestamp_4}",
                "language": "en-IN"
            },
            {
                "speaker": "AI",
                "text": "Thank you. Just to confirm, are you referring to day.",
                "timestamp": "{timestamp_5}",
                "language": "en-IN"
            },
            {
                "speaker": "User",
                "text": "I will be paying it on 12th February 2026.",
                "timestamp": "{timestamp_6}",
                "language": "en-IN"
            },
            {
                "speaker": "AI",
                "text": "Thank you for confirming the payment date. We will look forward.",
                "timestamp": "{timestamp_7}",
                "language": "en-IN"
            },
            {
                "speaker": "User",
                "text": "Okay. Okay. Thank you so much.",
                "timestamp": "{timestamp_8}",
                "language": "en-IN"
            },
            {
                "speaker": "AI",
                "text": "Thank you for your time. We appreciate you confirming the.",
                "timestamp": "{timestamp_9}",
                "language": "en-IN"
            },
            {
                "speaker": "User",
                "text": "Okay, Good Bye, Take care.",
                "timestamp": "{timestamp_10}",
                "language": "en-IN"
            },
            {
                "speaker": "AI",
                "text": "Thank you for your time. We look forward to receiving your payment on February 12, 2026.",
                "timestamp": "{timestamp_11}",
                "language": "en-IN"
            }
        ]
    },
    "hi-IN": {
        "call_uuid": "dummy-call-{uuid}",
        "start_time": "{start_time}",
        "end_time": "{end_time}",
        "duration_seconds": 45.32,
        "preferred_language": "hi-IN",
        "final_language": "hi-IN",
        "language_switches": 0,
        "language_history": [],
        "conversation": [
            {
                "speaker": "AI",
                "text": "‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Æ‡•à‡§Ç ‡§µ‡§ø‡§§‡•ç‡§§ ‡§è‡§ú‡•á‡§Ç‡§∏‡•Ä ‡§∏‡•á ‡§Ü‡§™‡§ï‡•á ‡§≤‡•ã‡§® ‡§≠‡•Å‡§ó‡§§‡§æ‡§® ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ï‡•â‡§≤ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡•Ä ‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§≠‡•Å‡§ó‡§§‡§æ‡§® ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§¨‡§§‡§æ‡§è‡§Ç?",
                "timestamp": "{timestamp_1}",
                "language": "hi-IN"
            },
            {
                "speaker": "User",
                "text": "‡§Æ‡•à‡§Ç ‡§™‡§∞‡§∏‡•ã‡§Ç ‡§™‡•á‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞ ‡§¶‡•Ç‡§Ç‡§ó‡§æ, ‡§†‡•Ä‡§ï ‡§π‡•à?",
                "timestamp": "{timestamp_2}",
                "language": "hi-IN"
            },
            {
                "speaker": "AI",
                "text": "‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶‡•§ ‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§§‡§æ‡§∞‡•Ä‡§ñ ‡§¨‡§§‡§æ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç?",
                "timestamp": "{timestamp_3}",
                "language": "hi-IN"
            },
            {
                "speaker": "User",
                "text": "‡§Æ‡•à‡§Ç 12 ‡§´‡§∞‡§µ‡§∞‡•Ä ‡§ï‡•ã ‡§≠‡•Å‡§ó‡§§‡§æ‡§® ‡§ï‡§∞ ‡§¶‡•Ç‡§Ç‡§ó‡§æ‡•§",
                "timestamp": "{timestamp_4}",
                "language": "hi-IN"
            },
            {
                "speaker": "AI",
                "text": "‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶‡•§ ‡§π‡§Æ 12 ‡§´‡§∞‡§µ‡§∞‡•Ä ‡§ï‡•ã ‡§Ü‡§™‡§ï‡•á ‡§≠‡•Å‡§ó‡§§‡§æ‡§® ‡§ï‡§æ ‡§á‡§Ç‡§§‡§ú‡§æ‡§∞ ‡§ï‡§∞‡•á‡§Ç‡§ó‡•á‡•§",
                "timestamp": "{timestamp_5}",
                "language": "hi-IN"
            },
            {
                "speaker": "User",
                "text": "‡§†‡•Ä‡§ï ‡§π‡•à, ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶‡•§",
                "timestamp": "{timestamp_6}",
                "language": "hi-IN"
            }
        ]
    },
    "ta-IN": {
        "call_uuid": "dummy-call-{uuid}",
        "start_time": "{start_time}",
        "end_time": "{end_time}",
        "duration_seconds": 48.15,
        "preferred_language": "ta-IN",
        "final_language": "ta-IN",
        "language_switches": 0,
        "language_history": [],
        "conversation": [
            {
                "speaker": "AI",
                "text": "‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç, ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ®‡Æø‡Æ§‡Æø ‡Æ®‡Æø‡Æ±‡ØÅ‡Æµ‡Æ©‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Æø‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æü‡Æ©‡Øç ‡Æö‡ØÜ‡Æ≤‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æ™‡Æ±‡Øç‡Æ±‡Æø ‡ÆÖ‡Æ¥‡Øà‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ§‡Æ±‡Øç‡Æ™‡Øã‡Æ§‡Øà‡ÆØ ‡Æï‡Æü‡Øç‡Æü‡Æ£ ‡Æ®‡Æø‡Æ≤‡Øà‡ÆØ‡Øà ‡Æ§‡ÆØ‡Æµ‡ØÅ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æï‡ØÇ‡Æ±‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç?",
                "timestamp": "{timestamp_1}",
                "language": "ta-IN"
            },
            {
                "speaker": "User",
                "text": "‡Æ®‡Ææ‡Æ©‡Øç ‡Æ®‡Ææ‡Æ≥‡Øà ‡ÆÆ‡Æ±‡ØÅ‡Æ®‡Ææ‡Æ≥‡Øç ‡Æö‡ØÜ‡Æ≤‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç, ‡Æö‡Æ∞‡Æø‡ÆØ‡Ææ?",
                "timestamp": "{timestamp_2}",
                "language": "ta-IN"
            },
            {
                "speaker": "AI",
                "text": "‡Æ®‡Æ©‡Øç‡Æ±‡Æø. ‡Æï‡ØÅ‡Æ±‡Æø‡Æ™‡Øç‡Æ™‡Æø‡Æü‡Øç‡Æü ‡Æ§‡Øá‡Æ§‡Æø‡ÆØ‡Øà ‡Æï‡ØÇ‡Æ± ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡ØÅ‡ÆÆ‡Ææ?",
                "timestamp": "{timestamp_3}",
                "language": "ta-IN"
            },
            {
                "speaker": "User",
                "text": "‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Æø‡Æ™‡Øç‡Æ∞‡Æµ‡Æ∞‡Æø 12 ‡ÆÖ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æö‡ØÜ‡Æ≤‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Øá‡Æ©‡Øç.",
                "timestamp": "{timestamp_4}",
                "language": "ta-IN"
            },
            {
                "speaker": "AI",
                "text": "‡Æ®‡Æ©‡Øç‡Æ±‡Æø. ‡Æ™‡Æø‡Æ™‡Øç‡Æ∞‡Æµ‡Æ∞‡Æø 12 ‡ÆÖ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æü‡Øç‡Æü‡Æ£‡Æ§‡Øç‡Æ§‡Æø‡Æ±‡Øç‡Æï‡Ææ‡Æï ‡Æï‡Ææ‡Æ§‡Øç‡Æ§‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç.",
                "timestamp": "{timestamp_5}",
                "language": "ta-IN"
            },
            {
                "speaker": "User",
                "text": "‡Æö‡Æ∞‡Æø, ‡Æ®‡Æ©‡Øç‡Æ±‡Æø.",
                "timestamp": "{timestamp_6}",
                "language": "ta-IN"
            }
        ]
    }
}


# ============================================================
# LANGUAGE MAPPING FUNCTION
# ============================================================

def normalize_language(language: str) -> str:
    """
    Convert language names to locale codes
    HINDI/Hindi/hindi -> hi-IN
    ENGLISH/English/english -> en-IN
    TAMIL/Tamil/tamil -> ta-IN
    """
    language_upper = language.upper().strip()
    
    # Internal map for common names and common typos
    language_map = {
        "HINDI": "hi-IN",
        "HIND": "hi-IN",
        "ENGLISH": "en-IN",
        "ENGISH": "en-IN",  # Common typo
        "ENGLSH": "en-IN",
        "TAMIL": "ta-IN",
        "TAML": "ta-IN",
        "EN": "en-IN",
        "HI": "hi-IN",
        "TA": "ta-IN",
        "EN-IN": "en-IN",
        "HI-IN": "hi-IN",
        "TA-IN": "ta-IN"
    }
    
    # 1. Exact map check
    if language_upper in language_map:
        return language_map[language_upper]
    
    # 2. Case-insensitive config check
    for config_key in settings.LANGUAGE_CONFIG.keys():
        if config_key.upper() == language_upper:
            return config_key
            
    # 3. Fuzzy prefix matching
    if language_upper.startswith("EN"):
        return "en-IN"
    if language_upper.startswith("HI"):
        return "hi-IN"
    if language_upper.startswith("TA"):
        return "ta-IN"
        
    return language_map.get(language_upper, language)


# ============================================================
# PYDANTIC MODELS
# ============================================================

class BorrowerInfo(BaseModel):
    """Single borrower information"""
    NO: str = Field(..., description="Unique identifier for the borrower")
    cell1: str = Field(..., description="Phone number with country code (e.g., +911234567890)")
    preferred_language: str = Field(default="en-IN", description="Preferred language: en-IN, hi-IN, or ta-IN")
    
    class Config:
        json_schema_extra = {
            "example": {
                "NO": "BRW123456",
                "cell1": "+911234567890",
                "preferred_language": "hi-IN"
            }
        }


class BulkCallRequest(BaseModel):
    """Request model for bulk calling"""
    borrowers: List[BorrowerInfo] = Field(..., description="List of borrowers to call")
    use_dummy_data: bool = Field(default=True, description="Use dummy conversations instead of making real calls (saves credits)")
    
    class Config:
        json_schema_extra = {
            "example": {
                "borrowers": [
                    {
                        "NO": "BRW123456",
                        "cell1": "+911234567890",
                        "preferred_language": "hi-IN"
                    },
                    {
                        "NO": "BRW789012",
                        "cell1": "+911987654321",
                        "preferred_language": "en-IN"
                    },
                    {
                        "NO": "BRW345678",
                        "cell1": "+911122334455",
                        "preferred_language": "ta-IN"
                    }
                ],
                "use_dummy_data": True
            }
        }


class SingleCallRequest(BaseModel):
    """Request model for single call"""
    to_number: str = Field(..., description="Phone number to call (with country code)")
    language: str = Field(default="en-IN", description="Preferred language: en-IN, hi-IN, or ta-IN")
    borrower_id: Optional[str] = Field(None, description="Optional borrower ID for tracking")
    use_dummy_data: bool = Field(default=True, description="Use dummy conversation instead of making real call (saves credits)")
    
    class Config:
        json_schema_extra = {
            "example": {
                "to_number": "+911234567890",
                "language": "hi-IN",
                "borrower_id": "BRW123456",
                "use_dummy_data": True
            }
        }


class CallResponse(BaseModel):
    """Response model for call initiation"""
    success: bool
    call_uuid: Optional[str] = None
    status: Optional[str] = None
    to_number: Optional[str] = None
    language: Optional[str] = None
    borrower_id: Optional[str] = None
    error: Optional[str] = None
    is_dummy: Optional[bool] = False
    transcript_file: Optional[str] = None
    ai_analysis: Optional[dict] = None
    conversation: Optional[List[dict]] = None


class BulkCallResponse(BaseModel):
    """Response model for bulk calling"""
    total_requests: int
    successful_calls: int
    failed_calls: int
    results: List[CallResponse]
    mode: str = "dummy"  # "dummy" or "real"


# ============================================================
# HELPER FUNCTION TO CREATE DUMMY CALL
# ============================================================

def create_dummy_call(phone_number: str, language: str, borrower_id: Optional[str] = None) -> dict:
    """
    Create a dummy call with simulated conversation and AI analysis
    This saves Vonage call credits while testing
    
    Args:
        phone_number: Phone number for the call
        language: Language code (en-IN, hi-IN, ta-IN)
        borrower_id: Optional borrower ID for tracking
    
    Returns:
        dict: Call result with UUID, status, and AI analysis
    """
    try:
        # Generate unique call UUID
        call_uuid = f"dummy-{uuid.uuid4()}"
        
        # Get dummy conversation template for the language
        if language not in DUMMY_CONVERSATIONS:
            return {
                "success": False,
                "error": f"No dummy conversation available for language: {language}"
            }
        
        # Clone the template
        conversation_template = DUMMY_CONVERSATIONS[language].copy()
        
        # Generate timestamps
        start_time = datetime.now()
        current_time = start_time
        
        # Replace placeholders in conversation
        conversation = []
        for i, entry in enumerate(conversation_template["conversation"]):
            # Add 3-8 seconds between messages
            seconds_to_add = random.uniform(3.0, 8.0)
            current_time = current_time + timedelta(seconds=seconds_to_add)
            
            conversation.append({
                "speaker": entry["speaker"],
                "text": entry["text"],
                "timestamp": current_time.isoformat(),
                "language": language
            })
        
        end_time = current_time
        duration = (end_time - start_time).total_seconds()
        
        # Perform AI analysis on the conversation
        print(f"[DUMMY CALL] ü§ñ Running AI analysis for call {call_uuid} (Borrower: {borrower_id})")
        ai_analysis = analyze_conversation_with_gemini(conversation)
        
        # Create transcript data
        transcript_data = {
            "call_uuid": call_uuid,
            "borrower_id": borrower_id,
            "phone_number": phone_number,
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": round(duration, 2),
            "preferred_language": language,
            "final_language": language,
            "language_switches": 0,
            "language_history": [],
            "conversation": conversation,
            "ai_analysis": ai_analysis,
            "is_dummy": True,
            "note": "This is a simulated conversation for testing purposes. No actual call was made."
        }
        
        # Save transcript to file (using hidden folder to avoid triggering frontend reloads)
        import os
        os.makedirs(".transcripts", exist_ok=True)
        filename = f".transcripts/transcript_{call_uuid}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        # Save to MongoDB (Call Session Schema)
        try:
            db.insert_call_session(transcript_data)
        except Exception as e:
            print(f"[DB] ‚ùå Failed to save Call Session: {e}")
        
        print(f"[DUMMY CALL] ‚úÖ Session saved to MongoDB")
        
        # Print AI analysis summary
        if ai_analysis:
            print(f"  üìä Sentiment: {ai_analysis.get('sentiment', 'N/A')} | Intent: {ai_analysis.get('intent', 'N/A')}")
        
        return {
            "success": True,
            "call_uuid": call_uuid,
            "status": "completed (dummy)",
            "to_number": phone_number,
            "language": language,
            "borrower_id": borrower_id,
            "is_dummy": True,
            "transcript_file": filename,
            "ai_analysis": ai_analysis,
            "conversation": conversation
        }
        
    except Exception as e:
        print(f"[DUMMY CALL] ‚ùå Error creating dummy call for {borrower_id}: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


def process_single_call(borrower: BorrowerInfo, use_dummy_data: bool, normalized_language: str) -> CallResponse:
    """
    Process a single call (dummy or real) for parallel execution
    
    Args:
        borrower: Borrower information
        use_dummy_data: Whether to use dummy data or make real call
        normalized_language: Normalized language code
    
    Returns:
        CallResponse: Result of the call
    """
    print(f"[PARALLEL] Processing borrower: {borrower.NO} ({normalized_language})")
    
    # Make the call (dummy or real)
    if use_dummy_data:
        call_result = create_dummy_call(
            phone_number=borrower.cell1,
            language=normalized_language,
            borrower_id=borrower.NO
        )
    else:
        call_result = make_outbound_call(
            to_number=borrower.cell1,
            language=normalized_language,
            borrower_id=borrower.NO
        )
    
    # Create response
    if call_result.get("success"):
        return CallResponse(
            success=True,
            call_uuid=call_result.get("call_uuid"),
            status=call_result.get("status"),
            to_number=borrower.cell1,
            language=normalized_language,
            borrower_id=borrower.NO,
            is_dummy=use_dummy_data,
            transcript_file=call_result.get("transcript_file"),
            ai_analysis=call_result.get("ai_analysis"),
            conversation=call_result.get("conversation")
        )
    else:
        return CallResponse(
            success=False,
            error=call_result.get("error"),
            to_number=borrower.cell1,
            language=normalized_language,
            borrower_id=borrower.NO,
            is_dummy=use_dummy_data
        )


# ============================================================
# API ENDPOINTS
# ============================================================

@router.get("/")
async def ai_calling_root():
    """AI Calling module root endpoint"""
    return {
        "message": "AI Calling Module",
        "status": "active",
        "supported_languages": list(settings.LANGUAGE_CONFIG.keys()),
        "features": [
            "Multi-language support (English, Hindi, Tamil)",
            "Automatic language detection",
            "AI-powered conversation analysis",
            "Bulk calling capability",
            "Real-time transcription",
            "Sentiment analysis",
            "Intent classification",
            "Dummy mode (saves call credits during testing)"
        ],
        "modes": {
            "dummy": "Use simulated conversations with AI analysis (no actual calls made)",
            "real": "Make actual calls via Vonage (uses call credits)"
        }
    }


@router.post("/trigger_calls", response_model=BulkCallResponse)
async def trigger_bulk_calls(
    request: BulkCallRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    Trigger multiple calls to borrowers with their preferred languages using PARALLEL PROCESSING
    """
    if not request.borrowers:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="No borrowers provided in the request"
        )
    
    mode = "DUMMY" if request.use_dummy_data else "REAL"
    total_borrowers = len(request.borrowers)
    
    print(f"\n{'='*60}")
    print(f"üìû BULK CALL REQUEST ({mode} MODE) - {total_borrowers} borrowers")
    print(f"üë§ User: {current_user.get('username')} | Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")
    
    # Start timing
    start_time = time.time()
    
    # Prepare tasks for parallel processing
    tasks = []
    
    for borrower in request.borrowers:
        # Normalize language
        normalized_language = normalize_language(borrower.preferred_language)
        
        # Validate language
        if normalized_language not in settings.LANGUAGE_CONFIG:
            tasks.append({
                "borrower": borrower,
                "normalized_language": normalized_language,
                "valid": False,
                "error": f"Unsupported language: {borrower.preferred_language}"
            })
        else:
            tasks.append({
                "borrower": borrower,
                "normalized_language": normalized_language,
                "valid": True
            })
    
    # Process calls in parallel
    results = []
    successful = 0
    failed = 0
    
    max_workers = min(10, total_borrowers)
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_task = {}
        for task in tasks:
            if not task["valid"]:
                result = CallResponse(
                    success=False,
                    error=task["error"],
                    to_number=task["borrower"].cell1,
                    borrower_id=task["borrower"].NO,
                    language=task["borrower"].preferred_language,
                    is_dummy=request.use_dummy_data
                )
                results.append(result)
                failed += 1
            else:
                future = executor.submit(
                    process_single_call,
                    task["borrower"],
                    request.use_dummy_data,
                    task["normalized_language"]
                )
                future_to_task[future] = task
        
    for future in as_completed(future_to_task):
            task = future_to_task[future]
            try:
                result = future.result()
                results.append(result)
                if result.success: 
                    successful += 1
                    print(f"[PARALLEL] ‚úÖ Success: {result.borrower_id}")
                else: 
                    failed += 1
                    print(f"[PARALLEL] ‚ùå Failed: {result.borrower_id} - {result.error}")
            except Exception as e:
                failed += 1
                print(f"[PARALLEL] üî• Critical Error for {task['borrower'].NO}: {e}")
    
    duration = round(time.time() - start_time, 2)
    print(f"\n{'='*60}")
    print(f"üìä BULK CALL COMPLETED in {duration}s")
    print(f"‚úÖ Success: {successful} | ‚ùå Failed: {failed}")
    print(f"{'='*60}\n")
    
    return BulkCallResponse(
        total_requests=total_borrowers,
        successful_calls=successful,
        failed_calls=failed,
        results=results,
        mode="dummy" if request.use_dummy_data else "real"
    )


@router.post("/make_call", response_model=CallResponse)
async def make_single_call(
    request: SingleCallRequest,
    current_user: dict = Depends(get_current_user)
):
    """Trigger a single AI-powered call"""
    normalized_language = normalize_language(request.language)
    
    if normalized_language not in settings.LANGUAGE_CONFIG:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Unsupported language"
        )
    
    if request.use_dummy_data:
        call_result = create_dummy_call(
            phone_number=request.to_number,
            language=normalized_language,
            borrower_id=request.borrower_id
        )
    else:
        call_result = make_outbound_call(
            to_number=request.to_number,
            language=normalized_language,
            borrower_id=request.borrower_id
        )
    
    if call_result.get("success"):
        return CallResponse(
            success=True,
            call_uuid=call_result.get("call_uuid"),
            status=call_result.get("status"),
            to_number=request.to_number,
            language=normalized_language,
            borrower_id=request.borrower_id,
            is_dummy=request.use_dummy_data,
            transcript_file=call_result.get("transcript_file"),
            ai_analysis=call_result.get("ai_analysis"),
            conversation=call_result.get("conversation")
        )
    else:
        return CallResponse(
            success=False,
            error=call_result.get("error"),
            to_number=request.to_number,
            language=normalized_language,
            borrower_id=request.borrower_id,
            is_dummy=request.use_dummy_data
        )


@router.get("/sessions/{loan_no}")
async def get_loan_sessions(
    loan_no: str,
    current_user: dict = Depends(get_current_user)
):
    """Get all call sessions for a specific loan number"""
    sessions = db.get_all_sessions_for_loan(loan_no)
    for s in sessions:
        s["_id"] = str(s["_id"])
    return sessions


@router.get("/session/{call_uuid}")
async def get_call_session(
    call_uuid: str,
    current_user: dict = Depends(get_current_user)
):
    """Get details of a specific call session by UUID"""
    session = db.get_call_session(call_uuid)
    if session:
        session["_id"] = str(session["_id"])
        return session
    
    raise HTTPException(status_code=404, detail="Session not found")


@router.get("/analysis/{call_uuid}")
async def get_analysis(
    call_uuid: str,
    current_user: dict = Depends(get_current_user)
):
    """Get only the AI analysis for a specific call session"""
    session = db.get_call_session(call_uuid)
    if session and 'ai_analysis' in session:
        return {
            "call_uuid": call_uuid,
            "loan_no": session.get("loan_no"),
            "is_dummy": session.get("is_dummy", False),
            "ai_analysis": session['ai_analysis']
        }
    
    raise HTTPException(status_code=404, detail="Analysis not found")


@router.get("/health")
async def health_check():
    """
    Health check endpoint for the AI calling service
    
    **Returns:**
    - Service status
    - Active calls count
    - Supported languages
    - Available features
    - Available modes (dummy/real)
    """
    
    call_data_store = get_call_data_store()
    
    return {
        "status": "healthy",
        "active_calls": len([h for h in call_data_store.values() if h.is_active]),
        "total_calls": len(call_data_store),
        "sarvam_ai": "STT/TTS (saarika:v2.5 + bulbul:v2)",
        "gemini_ai": "conversation analysis" if gemini_client else "not configured",
        "supported_languages": list(settings.LANGUAGE_CONFIG.keys()),
        "modes": {
            "dummy": "Simulated conversations with AI analysis (no call credits used)",
            "real": "Actual phone calls via Vonage (uses call credits)"
        },
        "features": [
            "Auto language detection",
            "Multi-language support (English, Hindi, Tamil)",
            "Real-time conversation",
            "Language switching",
            "AI-powered analysis (summary, sentiment, intent)",
            "Borrower intent classification",
            "Bulk calling support",
            "Dummy mode for testing"
        ]
    }


# Note: WebSocket endpoints and webhook endpoints should be handled separately
# in a Flask app or using FastAPI WebSocket support
# The following endpoints would need to be implemented in the Flask portion:
# - /webhooks/answer (POST) - for Vonage answer webhook
# - /webhooks/event (POST) - for Vonage event webhook
# - /socket/<call_uuid> (WebSocket) - for real-time audio streaming